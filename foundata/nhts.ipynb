{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2427d56",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "import polars as pl\n",
    "import yaml\n",
    "\n",
    "from nhts.trips import preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "569e093a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "column HH_RACE not in year 2001\n",
      "year 2022 is missing columns: {'HHR_RACE'}\n",
      "2022: 7893 hhs found\n",
      "year 2017 is missing columns: {'HHR_RACE'}\n",
      "2017: 129696 hhs found\n",
      "year 2009 is missing columns: {'HHR_RACE'}\n",
      "2009: 150147 hhs found\n",
      "year 2001 is missing columns: {'HH_RACE'}\n",
      "2001: 69817 hhs found\n"
     ]
    }
   ],
   "source": [
    "hhs = {}\n",
    "\n",
    "years = [2022, 2017, 2009, 2001]\n",
    "names = [\"hhv2pub.csv\", \"hhpub.csv\", \"HHV2PUB.CSV\", \"HHPUB.csv\"]\n",
    "\n",
    "with open(\"nhts/hh_dictionary.yaml\") as f:\n",
    "    config = yaml.safe_load(f)\n",
    "\n",
    "column_mapping = config[\"column_mappings\"]\n",
    "\n",
    "\n",
    "# check table keys\n",
    "\n",
    "for col in column_mapping.keys():\n",
    "    col_sets = []\n",
    "    for year, name in zip(years, names):\n",
    "        if col not in config:\n",
    "            continue  # skip non categorical columns\n",
    "\n",
    "        path = Path(\"~/Data/foundata/NHTS\") / str(year) / name\n",
    "        data = pl.read_csv(path, ignore_errors=True)\n",
    "\n",
    "        # select columns\n",
    "        if col not in data.columns:\n",
    "            print(f\"column {col} not in year {year}\")\n",
    "            continue\n",
    "\n",
    "    for i in range(1, len(col_sets)):\n",
    "        if col_sets[0] != col_sets[i]:\n",
    "            print(f\"mismatch in column {col}:\")\n",
    "            print(f\" 2022: {col_sets[0]} vs {years[i]}:  {col_sets[i]}\")\n",
    "\n",
    "\n",
    "for year, name in [\n",
    "    (2022, \"hhv2pub.csv\"),\n",
    "    (2017, \"hhpub.csv\"),\n",
    "    (2009, \"HHV2PUB.CSV\"),\n",
    "    (2001, \"HHPUB.csv\"),\n",
    "]:\n",
    "    path = Path(\"~/Data/foundata/NHTS\") / str(year) / name\n",
    "    data = pl.read_csv(path, ignore_errors=True)\n",
    "\n",
    "    # select columns\n",
    "    available_cols = set(data.columns)\n",
    "    required_cols = set(column_mapping.keys())\n",
    "    missing_cols = required_cols - available_cols\n",
    "    if missing_cols:\n",
    "        print(f\"year {year} is missing columns: {missing_cols}\")\n",
    "    column_mapping_filtered = {\n",
    "        k: v for k, v in column_mapping.items() if k in available_cols\n",
    "    }\n",
    "    cols = list(column_mapping_filtered.keys())\n",
    "    data = data.select(cols)\n",
    "\n",
    "    # apply mappings\n",
    "    data = data.with_columns(\n",
    "        pl.col(col)\n",
    "        .replace_strict(config[col].get(year, config[col][\"default\"]), default=None)\n",
    "        .fill_null(pl.col(col))\n",
    "        for col in column_mapping_filtered.keys()\n",
    "        if col in config\n",
    "    )\n",
    "\n",
    "    # rename columns\n",
    "    data = data.rename(column_mapping_filtered)\n",
    "\n",
    "    # split year and month\n",
    "    data = data.with_columns(\n",
    "        data[\"date\"].cast(pl.String).str.slice(0, 4).cast(pl.Int32).alias(\"year\"),\n",
    "        data[\"date\"].cast(pl.String).str.slice(4).cast(pl.Int32).alias(\"month\"),\n",
    "    )\n",
    "\n",
    "    # ensure unique household ids across years\n",
    "    data = data.with_columns((pl.col(\"hid\") + year * 10_000_000_000).alias(\"hid\"))\n",
    "\n",
    "    hhs[year] = data\n",
    "\n",
    "    print(f\"{year}: {len(hhs[year])} hhs found\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "214a885d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "2db8f536",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mismatch in column EDUC:\n",
      " 2022: {1, 2, 3, 4, 5, 6, 7, 8, -1} vs 2017:  {1, 2, 3, 4, 5, -8, -7, -1}\n",
      "mismatch in column EDUC:\n",
      " 2022: {1, 2, 3, 4, 5, 6, 7, 8, -1} vs 2009:  {1, 2, 3, 4, 5, -9, -8, -7, -1}\n",
      "mismatch in column EDUC:\n",
      " 2022: {1, 2, 3, 4, 5, 6, 7, 8, -1} vs 2001:  {1, 2, 3, 4, 5, 6, 7, 8, -9, -8, -7, -1}\n",
      "mismatch in column MEDCOND:\n",
      " 2022: {1, 2, -9, -8, -7} vs 2009:  {1, 2, -9, -8, -7, -1}\n",
      "mismatch in column MEDCOND:\n",
      " 2022: {1, 2, -9, -8, -7} vs 2001:  {1, 2, -9, -8, -7, -1}\n",
      "mismatch in column PRMACT:\n",
      " 2022: {1, 2, 3, 4, 5, 97, -1} vs 2017:  {1, 2, 3, 4, 5, 6, 97, -8, -7, -1}\n",
      "mismatch in column PRMACT:\n",
      " 2022: {1, 2, 3, 4, 5, 97, -1} vs 2009:  {1, 2, 3, 4, 5, 6, 7, -9, -8, -7, -1}\n",
      "mismatch in column PRMACT:\n",
      " 2022: {1, 2, 3, 4, 5, 97, -1} vs 2001:  {1, 2, 3, 4, 5, 6, 7, -9, -8, -7, -1}\n",
      "mismatch in column R_RELAT:\n",
      " 2022: {1, 2, 3, 4, 5, 6, 7, -9} vs 2017:  {1, 2, 3, 4, 5, 6, 7, -9, -8, -7}\n",
      "mismatch in column R_RELAT:\n",
      " 2022: {1, 2, 3, 4, 5, 6, 7, -9} vs 2009:  {1, 2, 3, 4, 5, 6, 7, 8, -9, -8, -7}\n",
      "mismatch in column R_RELAT:\n",
      " 2022: {1, 2, 3, 4, 5, 6, 7, -9} vs 2001:  {1, 2, 3, 4, 5, 6, 7, 8, -9, -8, -7}\n",
      "mismatch in column R_SEX:\n",
      " 2022: {1, 2, -9, -8, -7} vs 2017:  {-8, -7, 2, 1}\n",
      "mismatch in column R_SEX:\n",
      " 2022: {1, 2, -9, -8, -7} vs 2009:  {1, 2}\n",
      "mismatch in column R_SEX:\n",
      " 2022: {1, 2, -9, -8, -7} vs 2001:  {-8, -7, 2, 1}\n",
      "mismatch in column DRIVER:\n",
      " 2022: {1, 2, -1} vs 2009:  {1, 2, -1, -9}\n",
      "mismatch in column DRIVER:\n",
      " 2022: {1, 2, -1} vs 2001:  {1, 2, -1, -9}\n",
      "2022: 16997 persons found\n",
      "2017: 264234 persons found\n",
      "2009: 308901 persons found\n",
      "2001: 160758 persons found\n"
     ]
    }
   ],
   "source": [
    "persons = {}\n",
    "\n",
    "years = [2022, 2017, 2009, 2001]\n",
    "names = [\"perv2pub.csv\", \"perpub.csv\", \"PERV2PUB.CSV\", \"PERPUB.csv\"]\n",
    "\n",
    "with open(\"nhts/person_dictionary.yaml\") as f:\n",
    "    config = yaml.safe_load(f)\n",
    "\n",
    "column_mapping = config[\"column_mappings\"]\n",
    "\n",
    "\n",
    "# check table keys\n",
    "\n",
    "for col in column_mapping.keys():\n",
    "    col_sets = []\n",
    "    for year, name in zip(years, names):\n",
    "        if col not in config:\n",
    "            continue  # skip non categorical columns\n",
    "\n",
    "        path = Path(\"~/Data/foundata/NHTS\") / str(year) / name\n",
    "        data = pl.read_csv(path, ignore_errors=True)\n",
    "\n",
    "        # select columns\n",
    "        if col not in data.columns:\n",
    "            print(f\"column {col} not in year {year}\")\n",
    "            continue\n",
    "        col_sets.append(set(data[col].unique().to_list()))\n",
    "\n",
    "    for i in range(1, len(col_sets)):\n",
    "        if col_sets[0] != col_sets[i]:\n",
    "            print(f\"mismatch in column {col}:\")\n",
    "            print(f\" 2022: {col_sets[0]} vs {years[i]}:  {col_sets[i]}\")\n",
    "\n",
    "\n",
    "for year, name in zip(years, names):\n",
    "    path = Path(\"~/Data/foundata/NHTS\") / str(year) / name\n",
    "    data = pl.read_csv(path, ignore_errors=True)\n",
    "\n",
    "    # select columns\n",
    "    data = data.select(column_mapping.keys())\n",
    "\n",
    "    # apply mappings\n",
    "    data = data.with_columns(\n",
    "        pl.col(col)\n",
    "        .replace_strict(config[col].get(year, config[col][\"default\"]), default=None)\n",
    "        .fill_null(pl.col(col))\n",
    "        for col in column_mapping.keys()\n",
    "        if col in config\n",
    "    )\n",
    "\n",
    "    # rename columns\n",
    "    data = data.rename(column_mapping)\n",
    "\n",
    "    # ensure unique person ids across years\n",
    "    data = data.with_columns(\n",
    "        ((pl.col(\"hid\") + year * 10_000_000_000) * 10 + pl.col(\"phid\")).alias(\"pid\")\n",
    "    )\n",
    "\n",
    "    persons[year] = data\n",
    "\n",
    "    print(f\"{year}: {len(persons[year])} persons found\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "db4b267f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div><style>\n",
       ".dataframe > thead > tr,\n",
       ".dataframe > tbody > tr {\n",
       "  text-align: right;\n",
       "  white-space: pre-wrap;\n",
       "}\n",
       "</style>\n",
       "<small>shape: (5, 10)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>phid</th><th>hid</th><th>age</th><th>education</th><th>disability</th><th>employment_status</th><th>relationship_to_respondent</th><th>sex</th><th>has_license</th><th>pid</th></tr><tr><td>i64</td><td>i64</td><td>i64</td><td>str</td><td>str</td><td>str</td><td>str</td><td>str</td><td>str</td><td>i64</td></tr></thead><tbody><tr><td>1</td><td>9000013002</td><td>39</td><td>&quot;graduate_degree&quot;</td><td>&quot;no&quot;</td><td>&quot;unemployed&quot;</td><td>&quot;self&quot;</td><td>&quot;female&quot;</td><td>&quot;yes&quot;</td><td>202290000130021</td></tr><tr><td>2</td><td>9000013002</td><td>42</td><td>&quot;graduate_degree&quot;</td><td>&quot;no&quot;</td><td>&quot;employed&quot;</td><td>&quot;spouse/partner&quot;</td><td>&quot;male&quot;</td><td>&quot;yes&quot;</td><td>202290000130022</td></tr><tr><td>3</td><td>9000013002</td><td>8</td><td>&quot;unknown&quot;</td><td>&quot;no&quot;</td><td>&quot;employed&quot;</td><td>&quot;child&quot;</td><td>&quot;unknown&quot;</td><td>&quot;unknown&quot;</td><td>202290000130023</td></tr><tr><td>4</td><td>9000013002</td><td>5</td><td>&quot;unknown&quot;</td><td>&quot;no&quot;</td><td>&quot;employed&quot;</td><td>&quot;child&quot;</td><td>&quot;unknown&quot;</td><td>&quot;unknown&quot;</td><td>202290000130024</td></tr><tr><td>1</td><td>9000013016</td><td>32</td><td>&quot;bachelors_degree&quot;</td><td>&quot;yes&quot;</td><td>&quot;employed&quot;</td><td>&quot;self&quot;</td><td>&quot;female&quot;</td><td>&quot;yes&quot;</td><td>202290000130161</td></tr></tbody></table></div>"
      ],
      "text/plain": [
       "shape: (5, 10)\n",
       "┌──────┬────────────┬─────┬──────────────┬───┬──────────────┬─────────┬─────────────┬──────────────┐\n",
       "│ phid ┆ hid        ┆ age ┆ education    ┆ … ┆ relationship ┆ sex     ┆ has_license ┆ pid          │\n",
       "│ ---  ┆ ---        ┆ --- ┆ ---          ┆   ┆ _to_responde ┆ ---     ┆ ---         ┆ ---          │\n",
       "│ i64  ┆ i64        ┆ i64 ┆ str          ┆   ┆ nt           ┆ str     ┆ str         ┆ i64          │\n",
       "│      ┆            ┆     ┆              ┆   ┆ ---          ┆         ┆             ┆              │\n",
       "│      ┆            ┆     ┆              ┆   ┆ str          ┆         ┆             ┆              │\n",
       "╞══════╪════════════╪═════╪══════════════╪═══╪══════════════╪═════════╪═════════════╪══════════════╡\n",
       "│ 1    ┆ 9000013002 ┆ 39  ┆ graduate_deg ┆ … ┆ self         ┆ female  ┆ yes         ┆ 202290000130 │\n",
       "│      ┆            ┆     ┆ ree          ┆   ┆              ┆         ┆             ┆ 021          │\n",
       "│ 2    ┆ 9000013002 ┆ 42  ┆ graduate_deg ┆ … ┆ spouse/partn ┆ male    ┆ yes         ┆ 202290000130 │\n",
       "│      ┆            ┆     ┆ ree          ┆   ┆ er           ┆         ┆             ┆ 022          │\n",
       "│ 3    ┆ 9000013002 ┆ 8   ┆ unknown      ┆ … ┆ child        ┆ unknown ┆ unknown     ┆ 202290000130 │\n",
       "│      ┆            ┆     ┆              ┆   ┆              ┆         ┆             ┆ 023          │\n",
       "│ 4    ┆ 9000013002 ┆ 5   ┆ unknown      ┆ … ┆ child        ┆ unknown ┆ unknown     ┆ 202290000130 │\n",
       "│      ┆            ┆     ┆              ┆   ┆              ┆         ┆             ┆ 024          │\n",
       "│ 1    ┆ 9000013016 ┆ 32  ┆ bachelors_de ┆ … ┆ self         ┆ female  ┆ yes         ┆ 202290000130 │\n",
       "│      ┆            ┆     ┆ gree         ┆   ┆              ┆         ┆             ┆ 161          │\n",
       "└──────┴────────────┴─────┴──────────────┴───┴──────────────┴─────────┴─────────────┴──────────────┘"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "persons[2022].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0c62574",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "bcf3a737",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30844\n",
      "915050\n",
      "1060185\n",
      "472704\n"
     ]
    }
   ],
   "source": [
    "pops = {}\n",
    "\n",
    "with open(\"nhts/trip_dictionary.yaml\") as f:\n",
    "    config = yaml.safe_load(f)\n",
    "\n",
    "for year, name in [\n",
    "    (2022, \"tripv2pub.csv\"),\n",
    "    (2017, \"trippub.csv\"),\n",
    "    (2009, \"DAYV2PUB.CSV\"),\n",
    "    (2001, \"DAYPUB.csv\"),\n",
    "]:\n",
    "    path = Path(\"~/Data/foundata/NHTS\") / str(year) / name\n",
    "    trips = pl.read_csv(path, ignore_errors=True)\n",
    "    trips = preprocess(trips, year=year, config=config)\n",
    "    print(len(trips))\n",
    "    # pam_trips = read.load_travel_diary(\n",
    "    #     trips=trips.to_pandas()\n",
    "    # )\n",
    "    # pops[year] = pam_trips"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bc302285",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'BIKE2SAVE': {-9: 'Not ascertained',\n",
       "  -8: \"I don't know\",\n",
       "  -7: 'I prefer not to answer',\n",
       "  1: 'Strongly agree',\n",
       "  2: 'Agree',\n",
       "  3: 'Neither Agree or Disagree',\n",
       "  4: 'Disagree',\n",
       "  5: 'Strongly disagree'},\n",
       " 'HBHTNRNT': {-9: 'Not ascertained',\n",
       "  0: '0-4%',\n",
       "  5: '5-14%',\n",
       "  20: '15-24%',\n",
       "  30: '25-34%',\n",
       "  40: '35-44%',\n",
       "  50: '45-54%',\n",
       "  60: '55-64%',\n",
       "  70: '65-74%',\n",
       "  80: '75-84%',\n",
       "  90: '85-94%',\n",
       "  95: '95-100%'},\n",
       " 'HHRELATD': {1: 'Yes', 2: 'No'},\n",
       " 'CAR': {-8: \"I don't know\",\n",
       "  -7: 'I prefer not to answer',\n",
       "  -1: 'Appropriate skip',\n",
       "  1: 'Daily',\n",
       "  2: 'A few times a week',\n",
       "  3: 'A few times a month',\n",
       "  4: 'A few times a year',\n",
       "  5: 'Never'},\n",
       " 'MSASIZE': {1: 'In an MSA of Less than 250,000',\n",
       "  2: 'In an MSA of 250,000 - 499,999',\n",
       "  3: 'In an MSA of 500,000 - 999,999',\n",
       "  4: 'In an MSA or CMSA of 1,000,000 - 2,999,999',\n",
       "  5: 'In an MSA or CMSA of 3 million or more',\n",
       "  6: 'Not in MSA or CMSA'},\n",
       " 'HHRESP': {1: 'None', 2: 'None', 3: 'None', 4: 'None', 5: 'None', 7: 'None'},\n",
       " 'SPHONE': {-9: 'Not ascertained',\n",
       "  -8: \"I don't know\",\n",
       "  -7: 'I prefer not to answer',\n",
       "  1: 'Daily',\n",
       "  2: 'A few times a week',\n",
       "  3: 'A few times a month',\n",
       "  4: 'A few times a year',\n",
       "  5: 'Never'},\n",
       " 'HH_RACE': {-8: \"Don't know\",\n",
       "  -7: 'Refused',\n",
       "  1: 'White',\n",
       "  2: 'Black or African American',\n",
       "  3: 'Asian',\n",
       "  4: 'American Indian or Alaska Native',\n",
       "  5: 'Native Hawaiian or other Pacific Islander',\n",
       "  6: 'Multiple responses selected',\n",
       "  97: 'Some other race'},\n",
       " 'CENSUS_R': {1: 'Northeast', 2: 'Midwest', 3: 'South', 4: 'West'},\n",
       " 'SCRESP': {1: 'None'},\n",
       " 'WEBUSE17': {-9: 'Not ascertained',\n",
       "  -7: 'I prefer not to answer',\n",
       "  1: 'Daily',\n",
       "  2: 'A few times a week',\n",
       "  3: 'A few times a month',\n",
       "  4: 'A few times a year',\n",
       "  5: 'Never'},\n",
       " 'HBRESDN': {-9: 'Not ascertained',\n",
       "  50: '0-99',\n",
       "  300: '100-499',\n",
       "  750: '500-999',\n",
       "  1500: '1,000-1,999',\n",
       "  3000: '2,000-3,999',\n",
       "  7000: '4,000-9,999',\n",
       "  17000: '10,000-24,999',\n",
       "  30000: '25,000-999,999'},\n",
       " 'HBPPOPDN': {-9: 'Not ascertained',\n",
       "  50: '0-99',\n",
       "  300: '100-499',\n",
       "  750: '500-999',\n",
       "  1500: '1,000-1,999',\n",
       "  3000: '2,000-3,999',\n",
       "  7000: '4,000-9,999',\n",
       "  17000: '10,000-24,999',\n",
       "  30000: '25,000-999,999'},\n",
       " 'PARA': {-9: 'Not ascertained',\n",
       "  -8: \"I don't know\",\n",
       "  -7: 'I prefer not to answer',\n",
       "  1: 'Daily',\n",
       "  2: 'A few times a week',\n",
       "  3: 'A few times a month',\n",
       "  4: 'A few times a year',\n",
       "  5: 'Never'},\n",
       " 'HOMEOWN': {-8: \"I don't know\",\n",
       "  -7: 'I prefer not to answer',\n",
       "  1: 'Own',\n",
       "  2: 'Rent',\n",
       "  97: 'Some other arrangement'},\n",
       " 'TDAYDATE': {201604: 'None',\n",
       "  201605: 'None',\n",
       "  201606: 'None',\n",
       "  201607: 'None',\n",
       "  201608: 'None',\n",
       "  201609: 'None',\n",
       "  201610: 'None',\n",
       "  201611: 'None',\n",
       "  201612: 'None',\n",
       "  201701: 'None',\n",
       "  201702: 'None',\n",
       "  201703: 'None',\n",
       "  201704: 'None'},\n",
       " 'BIKE': {-9: 'Not ascertained',\n",
       "  -8: \"I don't know\",\n",
       "  -7: 'I prefer not to answer',\n",
       "  1: 'Daily',\n",
       "  2: 'A few times a week',\n",
       "  3: 'A few times a month',\n",
       "  4: 'A few times a year',\n",
       "  5: 'Never'},\n",
       " 'HHSTFIPS': {1: 'Alabama',\n",
       "  2: 'Alaska',\n",
       "  4: 'Arizona',\n",
       "  5: 'Arkansas',\n",
       "  6: 'California',\n",
       "  8: 'Colorado',\n",
       "  9: 'Connecticut',\n",
       "  10: 'Delaware',\n",
       "  11: 'District of Columbia',\n",
       "  12: 'Florida',\n",
       "  13: 'Georgia',\n",
       "  15: 'Hawaii',\n",
       "  16: 'Idaho',\n",
       "  17: 'Illinois',\n",
       "  18: 'Indiana',\n",
       "  19: 'Iowa',\n",
       "  20: 'Kansas',\n",
       "  21: 'Kentucky',\n",
       "  22: 'Louisiana',\n",
       "  23: 'Maine',\n",
       "  24: 'Maryland',\n",
       "  25: 'Massachusetts',\n",
       "  26: 'Michigan',\n",
       "  27: 'Minnesota',\n",
       "  28: 'Mississippi',\n",
       "  29: 'Missouri',\n",
       "  30: 'Montana',\n",
       "  31: 'Nebraska',\n",
       "  32: 'Nevada',\n",
       "  33: 'New Hampshire',\n",
       "  34: 'New Jersey',\n",
       "  35: 'New Mexico',\n",
       "  36: 'New York',\n",
       "  37: 'North Carolina',\n",
       "  38: 'North Dakota',\n",
       "  39: 'Ohio',\n",
       "  40: 'Oklahoma',\n",
       "  41: 'Oregon',\n",
       "  42: 'Pennsylvania',\n",
       "  44: 'Rhode Island',\n",
       "  45: 'South Carolina',\n",
       "  46: 'South Dakota',\n",
       "  47: 'Tennessee',\n",
       "  48: 'Texas',\n",
       "  49: 'Utah',\n",
       "  50: 'Vermont',\n",
       "  51: 'Virginia',\n",
       "  53: 'Washington',\n",
       "  54: 'West Virginia',\n",
       "  55: 'Wisconsin',\n",
       "  56: 'Wyoming'},\n",
       " 'URBAN': {1: 'In an urban area',\n",
       "  2: 'In an Urban cluster',\n",
       "  3: 'In an area surrounded by urban areas',\n",
       "  4: 'Not in urban area'},\n",
       " 'TRAVDAY': {1: 'Sunday',\n",
       "  2: 'Monday',\n",
       "  3: 'Tuesday',\n",
       "  4: 'Wednesday',\n",
       "  5: 'Thursday',\n",
       "  6: 'Friday',\n",
       "  7: 'Saturday'},\n",
       " 'TAXI': {-9: 'Not ascertained',\n",
       "  -8: \"I don't know\",\n",
       "  -7: 'I prefer not to answer',\n",
       "  1: 'Daily',\n",
       "  2: 'A few times a week',\n",
       "  3: 'A few times a month',\n",
       "  4: 'A few times a year',\n",
       "  5: 'Never'},\n",
       " 'CDIVMSAR': {11: 'New England (ME, NH, VT, CT, MA, RI) MSA or CMSA of 1 million or more with heavy rail',\n",
       "  12: 'New England (ME, NH, VT, CT, MA, RI) MSA or CMSA of 1 million or more without heavy rail',\n",
       "  13: 'New England (ME, NH, VT, CT, MA, RI) MSA of less than 1 million',\n",
       "  14: 'New England (ME, NH, VT, CT, MA, RI) Not in a MSA',\n",
       "  21: 'Mid-Atlantic (NY, NJ, PA) MSA or CMSA of 1 million or more with heavy rail',\n",
       "  22: 'Mid-Atlantic (NY, NJ, PA) MSA or CMSA of 1 million or more without heavy rail',\n",
       "  23: 'Mid-Atlantic (NY, NJ, PA) MSA of less than 1 million',\n",
       "  24: 'Mid-Atlantic (NY, NJ, PA) Not in a MSA',\n",
       "  31: 'East North Central (IL, IN, MI, OH, WI) MSA or CMSA of 1 million or more with heavy rail',\n",
       "  32: 'East North Central (IL, IN, MI, OH, WI) MSA or CMSA of 1 million or more without heavy rail',\n",
       "  33: 'East North Central (IL, IN, MI, OH, WI) MSA of less than 1 million',\n",
       "  34: 'East North Central (IL, IN, MI, OH, WI) Not in a MSA',\n",
       "  41: 'West North Central (IA, KS, MO, MN, ND, NE, SD) MSA or CMSA of 1 million or more with heavy rail',\n",
       "  42: 'West North Central (IA, KS, MO, MN, ND, NE, SD) MSA or CMSA of 1 million or more without heavy rail',\n",
       "  43: 'West North Central (IA, KS, MO, MN, ND, NE, SD) MSA of less than 1 million',\n",
       "  44: 'West North Central (IA, KS, MO, MN, ND, NE, SD) Not in a MSA',\n",
       "  51: 'South Atlantic (DE, FL, GA, MD, NC, SC, WV, VA) MSA or CMSA of 1 million or more with heavy rail',\n",
       "  52: 'South Atlantic (DE, FL, GA, MD, NC, SC, WV, VA) MSA or CMSA of 1 million or more without heavy rail',\n",
       "  53: 'South Atlantic (DE, FL, GA, MD, NC, SC, WV, VA) MSA of less than 1 million',\n",
       "  54: 'South Atlantic (DE, FL, GA, MD, NC, SC, WV, VA) Not in a MSA',\n",
       "  62: 'East South Central (AL, KY, MS, TN) MSA or CMSA of 1 million or more without heavy rail',\n",
       "  63: 'East South Central (AL, KY, MS, TN) MSA of less than 1 million',\n",
       "  64: 'East South Central (AL, KY, MS, TN) Not in a MSA',\n",
       "  71: 'West South Central (AR, LA, OK, TX) MSA or CMSA of 1 million or more with heavy rail',\n",
       "  72: 'West South Central (AR, LA, OK, TX) MSA or CMSA of 1 million or more without heavy rail',\n",
       "  73: 'West South Central (AR, LA, OK, TX) MSA of less than 1 million',\n",
       "  74: 'West South Central (AR, LA, OK, TX) Not in a MSA',\n",
       "  81: 'Mountain (AZ, CO, ID, MT, NM, NV, UT, WY) MSA or CMSA of 1 million or more with heavy rail',\n",
       "  82: 'Mountain (AZ, CO, ID, MT, NM, NV, UT, WY) MSA or CMSA of 1 million or more without heavy rail',\n",
       "  83: 'Mountain (AZ, CO, ID, MT, NM, NV, UT, WY) MSA of less than 1 million',\n",
       "  84: 'Mountain (AZ, CO, ID, MT, NM, NV, UT, WY) Not in a MSA',\n",
       "  91: 'Pacific (AK, CA, HI, OR, WA) MSA or CMSA of 1 million or more with heavy rail',\n",
       "  92: 'Pacific (AK, CA, HI, OR, WA) MSA or CMSA of 1 million or more without heavy rail',\n",
       "  93: 'Pacific (AK, CA, HI, OR, WA) MSA of less than 1 million',\n",
       "  94: 'Pacific (AK, CA, HI, OR, WA) Not in a MSA'},\n",
       " 'WALK': {-9: 'Not ascertained',\n",
       "  -8: \"I don't know\",\n",
       "  -7: 'I prefer not to answer',\n",
       "  1: 'Daily',\n",
       "  2: 'A few times a week',\n",
       "  3: 'A few times a month',\n",
       "  4: 'A few times a year',\n",
       "  5: 'Never'},\n",
       " 'PRICE': {-9: 'Not ascertained',\n",
       "  -8: \"I don't know\",\n",
       "  -7: 'I prefer not to answer',\n",
       "  1: 'Strongly agree',\n",
       "  2: 'Agree',\n",
       "  3: 'Neither Agree or Disagree',\n",
       "  4: 'Disagree',\n",
       "  5: 'Strongly disagree'},\n",
       " 'LIF_CYC': {-9: 'Not ascertained',\n",
       "  1: 'one adult, no children',\n",
       "  2: '2+ adults, no children',\n",
       "  3: 'one adult, youngest child 0-5',\n",
       "  4: '2+ adults, youngest child 0-5',\n",
       "  5: 'one adult, youngest child 6-15',\n",
       "  6: '2+ adults, youngest child 6-15',\n",
       "  7: 'one adult, youngest child 16-21',\n",
       "  8: '2+ adults, youngest child 16-21',\n",
       "  9: 'one adult, retired, no children',\n",
       "  10: '2+ adults, retired, no children'},\n",
       " 'HBHUR': {-9: 'Not ascertained'},\n",
       " 'HTHTNRNT': {-9: 'Not ascertained',\n",
       "  0: '0-4%',\n",
       "  5: '5-14%',\n",
       "  20: '15-24%',\n",
       "  30: '25-34%',\n",
       "  40: '35-44%',\n",
       "  50: '45-54%',\n",
       "  60: '55-64%',\n",
       "  70: '65-74%',\n",
       "  80: '75-84%',\n",
       "  90: '85-94%',\n",
       "  95: '95-100%'},\n",
       " 'TAB': {-9: 'Not ascertained',\n",
       "  -8: \"I don't know\",\n",
       "  -7: 'I prefer not to answer',\n",
       "  1: 'Daily',\n",
       "  2: 'A few times a week',\n",
       "  3: 'A few times a month',\n",
       "  4: 'A few times a year',\n",
       "  5: 'Never'},\n",
       " 'URBRUR': {1: 'Urban', 2: 'Rural'},\n",
       " 'PLACE': {-9: 'Not ascertained',\n",
       "  -8: \"I don't know\",\n",
       "  -7: 'I prefer not to answer',\n",
       "  1: 'Strongly agree',\n",
       "  2: 'Agree',\n",
       "  3: 'Neither Agree or Disagree',\n",
       "  4: 'Disagree',\n",
       "  5: 'Strongly disagree'},\n",
       " 'HTRESDN': {-9: 'Not ascertained',\n",
       "  50: '0-99',\n",
       "  300: '100-499',\n",
       "  750: '500-999',\n",
       "  1500: '1,000-1,999',\n",
       "  3000: '2,000-3,999',\n",
       "  7000: '4,000-9,999',\n",
       "  17000: '10,000-24,999',\n",
       "  30000: '25,000-999,999'},\n",
       " 'HTPPOPDN': {-9: 'Not ascertained',\n",
       "  50: '0-99',\n",
       "  300: '100-499',\n",
       "  750: '500-999',\n",
       "  1500: '1,000-1,999',\n",
       "  3000: '2,000-3,999',\n",
       "  7000: '4,000-9,999',\n",
       "  17000: '10,000-24,999',\n",
       "  30000: '25,000-999,999'},\n",
       " 'HTEEMPDN': {-9: 'Not ascertained',\n",
       "  25: '0-49',\n",
       "  75: '50-99',\n",
       "  150: '100-249',\n",
       "  350: '250-499',\n",
       "  750: '500-999',\n",
       "  1500: '1,000-1,999',\n",
       "  3000: '2,000-3,999',\n",
       "  5000: '4,000-999,999'},\n",
       " 'RAIL': {1: 'MSA has rail', 2: 'MSA does not have rail, or hh not in an MSA'},\n",
       " 'HHFAMINC': {-9: 'Not ascertained',\n",
       "  -8: \"I don't know\",\n",
       "  -7: 'I prefer not to answer',\n",
       "  1: 'Less than $10,000',\n",
       "  2: '$10,000 to $14,999',\n",
       "  3: '$15,000 to $24,999',\n",
       "  4: '$25,000 to $34,999',\n",
       "  5: '$35,000 to $49,999',\n",
       "  6: '$50,000 to $74,999',\n",
       "  7: '$75,000 to $99,999',\n",
       "  8: '$100,000 to $124,999',\n",
       "  9: '$125,000 to $149,999',\n",
       "  10: '$150,000 to $199,999',\n",
       "  11: '$200,000 or more'},\n",
       " 'SMPLSRCE': {1: 'National sample', 2: 'Add-on sample'},\n",
       " 'WALK2SAVE': {-9: 'Not ascertained',\n",
       "  -8: \"I don't know\",\n",
       "  -7: 'I prefer not to answer',\n",
       "  1: 'Strongly agree',\n",
       "  2: 'Agree',\n",
       "  3: 'Neither Agree or Disagree',\n",
       "  4: 'Disagree',\n",
       "  5: 'Strongly disagree'},\n",
       " 'HH_CBSA': {12060: 'Atlanta-Sandy Springs-Roswell, GA',\n",
       "  12420: 'Austin-Round Rock, TX',\n",
       "  12580: 'Baltimore-Columbia-Towson, MD',\n",
       "  13820: 'Birmingham-Hoover, AL',\n",
       "  14460: 'Boston-Cambridge-Newton, MA-NH',\n",
       "  15380: 'Buffalo-Cheektowaga-Niagara Falls, NY',\n",
       "  16740: 'Charlotte-Concord-Gastonia, NC-SC',\n",
       "  16980: 'Chicago-Naperville-Elgin, IL-IN-WI',\n",
       "  17140: 'Cincinnati, OH-KY-IN',\n",
       "  17460: 'Cleveland-Elyria, OH',\n",
       "  18140: 'Columbus, OH',\n",
       "  19100: 'Dallas-Fort Worth-Arlington, TX',\n",
       "  19740: 'Denver-Aurora-Lakewood, CO',\n",
       "  19820: 'Detroit-Warren-Dearborn, MI',\n",
       "  24340: 'Grand Rapids-Wyoming, MI',\n",
       "  25540: 'Hartford-West Hartford-East Hartford, CT',\n",
       "  26420: 'Houston-The Woodlands-Sugar Land, TX',\n",
       "  26900: 'Indianapolis-Carmel-Anderson, IN',\n",
       "  27260: 'Jacksonville, FL',\n",
       "  28140: 'Kansas City, MO-KS',\n",
       "  29820: 'Las Vegas-Henderson-Paradise, NV',\n",
       "  31080: 'Los Angeles-Long Beach-Anaheim, CA',\n",
       "  31140: 'Louisville/Jefferson County, KY-IN',\n",
       "  32820: 'Memphis, TN-MS-AR',\n",
       "  33100: 'Miami-Fort Lauderdale-West Palm Beach, FL',\n",
       "  33340: 'Milwaukee-Waukesha-West Allis, WI',\n",
       "  33460: 'Minneapolis-St. Paul-Bloomington, MN-WI',\n",
       "  34980: 'Nashville-Davidson--Murfreesboro--Franklin, TN',\n",
       "  35380: 'New Orleans-Metairie, LA',\n",
       "  35620: 'New York-Newark-Jersey City, NY-NJ-PA',\n",
       "  36420: 'Oklahoma City, OK',\n",
       "  36740: 'Orlando-Kissimmee-Sanford, FL',\n",
       "  37980: 'Philadelphia-Camden-Wilmington, PA-NJ-DE-MD',\n",
       "  38060: 'Phoenix-Mesa-Scottsdale, AZ',\n",
       "  38300: 'Pittsburgh, PA',\n",
       "  38900: 'Portland-Vancouver-Hillsboro, OR-WA',\n",
       "  39300: 'Providence-Warwick, RI-MA',\n",
       "  39580: 'Raleigh, NC',\n",
       "  40060: 'Richmond, VA',\n",
       "  40140: 'Riverside-San Bernardino-Ontario, CA',\n",
       "  40380: 'Rochester, NY',\n",
       "  40900: 'Sacramento--Roseville--Arden-Arcade, CA',\n",
       "  41180: 'St. Louis, MO-IL',\n",
       "  41620: 'Salt Lake City, UT',\n",
       "  41700: 'San Antonio-New Braunfels, TX',\n",
       "  41740: 'San Diego-Carlsbad, CA',\n",
       "  41860: 'San Francisco-Oakland-Hayward, CA',\n",
       "  41940: 'San Jose-Sunnyvale-Santa Clara, CA',\n",
       "  42660: 'Seattle-Tacoma-Bellevue, WA',\n",
       "  45300: 'Tampa-St. Petersburg-Clearwater, FL',\n",
       "  47260: 'Virginia Beach-Norfolk-Newport News, VA-NC',\n",
       "  47900: 'Washington-Arlington-Alexandria, DC-VA-MD-WV'},\n",
       " 'URBANSIZE': {1: '50,000 - 199,999',\n",
       "  2: '200,000 - 499,999',\n",
       "  3: '500,000 - 999,999',\n",
       "  4: '1 million or more without heavy rail',\n",
       "  5: '1 million or more with heavy rail',\n",
       "  6: 'Not in an urbanized area'},\n",
       " 'MSACAT': {1: 'MSA of 1 million or more, with rail',\n",
       "  2: 'MSA of 1 million or more, and not in 1',\n",
       "  3: 'MSA less than 1 million',\n",
       "  4: 'Not in MSA'},\n",
       " 'TRAIN': {-9: 'Not ascertained',\n",
       "  -8: \"I don't know\",\n",
       "  -7: 'I prefer not to answer',\n",
       "  1: 'Daily',\n",
       "  2: 'A few times a week',\n",
       "  3: 'A few times a month',\n",
       "  4: 'A few times a year',\n",
       "  5: 'Never'},\n",
       " 'CENSUS_D': {1: 'New England',\n",
       "  2: 'Middle Atlantic',\n",
       "  3: 'East North Central',\n",
       "  4: 'West North Central',\n",
       "  5: 'South Atlantic',\n",
       "  6: 'East South Central',\n",
       "  7: 'West South Central',\n",
       "  8: 'Mountain',\n",
       "  9: 'Pacific'},\n",
       " 'PTRANS': {-9: 'Not ascertained',\n",
       "  -8: \"I don't know\",\n",
       "  -7: 'I prefer not to answer',\n",
       "  1: 'Strongly agree',\n",
       "  2: 'Agree',\n",
       "  3: 'Neither Agree or Disagree',\n",
       "  4: 'Disagree',\n",
       "  5: 'Strongly disagree'},\n",
       " 'BUS': {-9: 'Not ascertained',\n",
       "  -8: \"I don't know\",\n",
       "  -7: 'I prefer not to answer',\n",
       "  1: 'Daily',\n",
       "  2: 'A few times a week',\n",
       "  3: 'A few times a month',\n",
       "  4: 'A few times a year',\n",
       "  5: 'Never'},\n",
       " 'PC': {-9: 'Not ascertained',\n",
       "  -8: \"I don't know\",\n",
       "  -7: 'I prefer not to answer',\n",
       "  1: 'Daily',\n",
       "  2: 'A few times a week',\n",
       "  3: 'A few times a month',\n",
       "  4: 'A few times a year',\n",
       "  5: 'Never'},\n",
       " 'HH_HISP': {-8: \"Don't Know\", -7: 'Refused', 1: 'Yes', 2: 'No'},\n",
       " 'SAMPSTRAT': {1: 'County in MSA with >= 1M and Heavy Rail',\n",
       "  2: 'County in MSA with >= 1M and no Heavy Rail',\n",
       "  3: 'County in MSA with < 1M',\n",
       "  4: 'County not in MSA'}}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "root = Path(\"~/Data/foundata/NHTS/2017\")\n",
    "codesheet = pl.read_excel(root / \"codebook_v1.2.xlsx\", sheet_name=\"CODEBOOK_HH\")\n",
    "\n",
    "codesheet = (\n",
    "    codesheet.with_columns(\n",
    "        [\n",
    "            pl.when(pl.col(c) == \"\").then(None).otherwise(pl.col(c)).alias(c)\n",
    "            for c in codesheet.columns\n",
    "            if codesheet[c].dtype == pl.String\n",
    "        ]\n",
    "    )\n",
    "    .fill_null(strategy=\"forward\")\n",
    "    .filter(pl.col(\"Type\") == \"C\")\n",
    "    .with_columns(\n",
    "        [\n",
    "            pl.col(\"Code / Range\")\n",
    "            .str.splitn(\"=\", 2)\n",
    "            .struct.field(\"field_0\")\n",
    "            .alias(\"key\"),\n",
    "            pl.col(\"Code / Range\")\n",
    "            .str.splitn(\"=\", 2)\n",
    "            .struct.field(\"field_1\")\n",
    "            .alias(\"value\"),\n",
    "        ]\n",
    "    )\n",
    "    .filter(pl.col(\"key\").cast(pl.Int64, strict=False).is_not_null())\n",
    "    .with_columns(key=pl.col(\"key\").cast(pl.Int32))\n",
    ")\n",
    "\n",
    "mapper = {}\n",
    "for i, frame in codesheet.group_by(\"Name\"):\n",
    "    column = str(i[0])\n",
    "    mapper[column] = {}\n",
    "    keys = frame.select(pl.col(\"key\")).rows()\n",
    "    values = frame.select(pl.col(\"value\")).rows()\n",
    "    for k, v in zip(keys, values):\n",
    "        mapper[column][int(k[0])] = str(v[0])\n",
    "\n",
    "mapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35a8aa14",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "foundata",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
